{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer, we use gpt-2 as example\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to the numeric tokens that the model could take as input\n",
    "tokens = tokenizer(['What should I do on a rainy day without an umbralla?'], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2061,   815,   314,   466,   319,   257, 37259,  1110,  1231,   281,\n",
       "         23781,  1671, 30315,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in necessary parameter 'input_ids' and got the ditectory output\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 50257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For CausalModel, we will got output with logits whose dimension is [batch_size, sequence_length, vocab_size(embedding dimension)]\n",
    "# - batch_size: represents the number of sequence in a batch, which is 1 for our example \n",
    "# - sequence_length: represents the length of output sequence which should be the same with input sequence, the length is the number of tokens\n",
    "# - vocab_size: more general, the third dimension is embedding dimension, but for LLM transformer model, it's the vaculary size, each element presents the probability of corresponding encoded token.\n",
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  318,   356,   466,    30,   616,  4445,  1110,    30,   257, 25510,\n",
       "           457,   496,    30,   198]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If pick up the biggest one in third dimension, we got the generated output tokens\n",
    "argmax_tokens = output.logits.argmax(axis=-1)\n",
    "argmax_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input tokens:  2061,   815,   314,   466,   319,   257, 37259,  1110,  1231,   281, 23781,  1671, 30315,  30\n",
    "\n",
    "output tokens:  318,   356,   466,    30,   616,  4445,  1110,    30,   257, 25510,   457,   496,    30,  198\n",
    "\n",
    "each token in output tokens, presents generated token based on the only access the tokens positioned before it in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(198)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We got one new generate token as continuation each time\n",
    "argmax_tokens[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_tokens: tensor([[ 2061,   815,   314,   466,   319,   257, 37259,  1110,  1231,   281,\n",
      "         23781,  1671, 30315,    30,   198,   198,   464,  3280,   318,    25,\n",
      "           645,    13]])\n",
      "generated_new_tokens: tensor([ 198,  198,  464, 3280,  318,   25,  645,   13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What should I do on a rainy day without an umbralla?\\n\\nThe answer is: no.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to do text generation, we got call the generate() interface instead calculate by ourself\n",
    "# The following code generate new 8 tokens\n",
    "generated_tokens = model.generate(**tokens, max_new_tokens=8, do_sample=False)\n",
    "generated_new_tokens = generated_tokens[0][len(tokens['input_ids'][0]):]\n",
    "print(f'generated_tokens: {generated_tokens}')\n",
    "print(f'generated_new_tokens: {generated_new_tokens}')\n",
    "tokenizer.decode(*generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens length: 15\n",
      "tensor([198])\n",
      "input tokens length: 16\n",
      "tensor([198])\n",
      "input tokens length: 17\n",
      "tensor([464])\n",
      "input tokens length: 18\n",
      "tensor([3280])\n",
      "input tokens length: 19\n",
      "tensor([318])\n",
      "input tokens length: 20\n",
      "tensor([25])\n",
      "input tokens length: 21\n",
      "tensor([645])\n",
      "input tokens length: 22\n",
      "tensor([13])\n"
     ]
    }
   ],
   "source": [
    "# If you want to implement that generate and decode logic by yourself, you could do as following\n",
    "def generate_one_new_token(input_tokens, model):\n",
    "    generated_tokens = model(input_tokens).logits.argmax(-1)\n",
    "    # Our batch_size is 1, so take the first batch and last position of sequence\n",
    "    generated_token_see_all_input = generated_tokens[0][-1:]\n",
    "    updated_input_tokens = torch.concat((input_tokens[0], generated_token_see_all_input), dim=0).unsqueeze(0)\n",
    "    print(f'input tokens length: {len(updated_input_tokens[0])}')\n",
    "    return generated_token_see_all_input, updated_input_tokens\n",
    "\n",
    "updated_input_tokens = tokens['input_ids']\n",
    "for i in range(0, 8):\n",
    "    generated_new_token, updated_input_tokens = generate_one_new_token(updated_input_tokens, model)\n",
    "    print(generated_new_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
